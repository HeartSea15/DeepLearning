"""
    ä½œè€…ï¼šHeart Sea
    åŠŸèƒ½ï¼šnpå®ç°å•éšå±‚çš„BPç®—æ³•ï¼Œç”¨åˆ°éªŒè¯é›†, æ­£åˆ™åŒ–
    Model:
        1ä¸ªè¾“å…¥å±‚
        å…¨è¿æ¥
        1ä¸ªéšå«å±‚, æ¿€æ´»ReLU,
        å…¨è¿æ¥
        1ä¸ªè¾“å‡ºå±‚, æ¿€æ´»ReLU, æŸå¤±å‡½æ•°ä¸ºäº¤å‰ç†µæŸå¤±

    åœ¨3.0çš„åŸºç¡€ä¸Šï¼Œæ”¹è¿›å¦‚ä¸‹ï¼š

        æŸå¤±å‡½æ•°ç”±äºŒæ¬¡ä»£ä»·å‡½æ•°æ”¹ä¸ºäº¤å‰ç†µæŸå¤±:é¿å…äº†å½“å‡ºç°â€œä¸¥é‡é”™è¯¯â€æ—¶å¯¼è‡´çš„å­¦ä¹ ç¼“æ…¢
        L2æ­£åˆ™åŒ–:æ”¹å–„è¿‡æ‹Ÿåˆ

        æƒå€¼åˆå§‹åŒ–:æƒå€¼åˆå§‹åŒ–:è®¾å¯¹lå±‚æœ‰æœ‰nä¸ªè¾“å…¥ç¥ç»å…ƒ,ä½¿ç”¨å‡å€¼ä¸º0,æ ‡å‡†å·®ä¸º 1/sqrt(n)çš„é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ–lå±‚çš„æƒå€¼
                æ”¹å–„ç½‘ç»œçš„å­¦ä¹ é€Ÿåº¦

        æ—©åœæ­¢ï¼šæ¯ä¸ªepochç»“æŸåï¼ˆæˆ–æ¯Nä¸ªepochåï¼‰ï¼š åœ¨éªŒè¯é›†ä¸Šè·å–æµ‹è¯•ç»“æœï¼Œ
            éšç€epochçš„å¢åŠ ï¼Œå¦‚æœåœ¨éªŒè¯é›†ä¸Šå‘ç°æµ‹è¯•è¯¯å·®ä¸Šå‡ï¼Œåˆ™åœæ­¢è®­ç»ƒï¼›
            å°†åœæ­¢ä¹‹åçš„æƒé‡ä½œä¸ºç½‘ç»œçš„æœ€ç»ˆå‚æ•°

            å› ä¸ºç²¾åº¦éƒ½ä¸å†æé«˜äº†ï¼Œåœ¨ç»§ç»­è®­ç»ƒä¹Ÿæ˜¯æ— ç›Šçš„ï¼Œåªä¼šæé«˜è®­ç»ƒçš„æ—¶é—´ã€‚
            é‚£ä¹ˆè¯¥åšæ³•çš„ä¸€ä¸ªé‡ç‚¹ä¾¿æ˜¯æ€æ ·æ‰è®¤ä¸ºéªŒè¯é›†ç²¾åº¦ä¸å†æé«˜äº†å‘¢?
            å¹¶ä¸æ˜¯è¯´éªŒè¯é›†ç²¾åº¦ä¸€é™ä¸‹æ¥ä¾¿è®¤ä¸ºä¸å†æé«˜äº†ï¼Œå› ä¸ºå¯èƒ½ç»è¿‡è¿™ä¸ªEpochåï¼Œç²¾åº¦é™ä½äº†ï¼Œ
            ä½†æ˜¯éšåçš„Epochåˆè®©ç²¾åº¦åˆä¸Šå»äº†ï¼Œæ‰€ä»¥ä¸èƒ½æ ¹æ®ä¸€ä¸¤æ¬¡çš„è¿ç»­é™ä½å°±åˆ¤æ–­ä¸å†æé«˜ã€‚
            ä¸€èˆ¬çš„åšæ³•æ˜¯ï¼Œåœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œè®°å½•åˆ°ç›®å‰ä¸ºæ­¢æœ€å¥½çš„éªŒè¯é›†ç²¾åº¦ï¼Œ
            å½“è¿ç»­10æ¬¡Epochï¼ˆæˆ–è€…æ›´å¤šæ¬¡ï¼‰æ²¡è¾¾åˆ°æœ€ä½³ç²¾åº¦æ—¶ï¼Œåˆ™å¯ä»¥è®¤ä¸ºç²¾åº¦ä¸å†æé«˜äº†ã€‚

    ç‰ˆæœ¬ä¸º3.0 çš„æµ‹è¯•é›†å‡†ç¡®ç‡95.14%
    ç‰ˆæœ¬ï¼š4.0 çš„æµ‹è¯•é›†å‡†ç¡®ç‡96.02%ï¼ˆè¿˜å¯ä»¥ç»§ç»­ä¼˜åŒ–ï¼‰
    æ—¥æœŸï¼š10/19/2019
    å‚è€ƒé“¾æ¥ï¼šhttp://neuralnetworksanddeeplearning.com/chap1.html
             https://www.jianshu.com/p/f9a14cec352d

    æœ¬ä»£ç æ‰§è¡Œé¡ºåºï¼š  åˆ‡åˆ†æ•°æ®é›†ï¼Œ
                    è®­ç»ƒï¼ˆç”¨åˆ°è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼‰ï¼Œ
                    ä¿å­˜æ¨¡å‹å’Œå‚æ•°ï¼Œ
                    è°ƒç”¨æ¨¡å‹å’Œå‚æ•°ï¼Œ
                    æµ‹è¯•é›†
"""

import gzip
import pickle
import random
import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt
import json
import sys


# åŠ è½½MNISTæ•°æ®é›†
def load_data():
    f = gzip.open('./data/mnist.pkl.gz')
    training_data, validation_data, test_data = pickle.load(f, encoding="latin1")
    f.close()
    return training_data, validation_data, test_data

# (50000, 784) (50000,)
# (10000, 784) (10000,)
# (10000, 784) (10000,)

# plt.imshow(image)
# plt.show()

# print(training_data[1][0])   # è®­ç»ƒé›†labelçš„ç¬¬ä¸€ä¸ªæ•°å­—5


#----------------æ„å»ºæ”¯æŒå‘é‡æœºSVMæ¨¡å‹ï¼Œä½œä¸ºäººå·¥ç¥ç»ç½‘ç»œæ¨¡å‹çš„æ€§èƒ½å¯¹æ¯”çš„åŸºæ¨¡å‹-------
# ä»¥svmæ¨¡å‹ä¸ºåŸºæ¨¡å‹ï¼ˆä¹Ÿå¯ä»¥ç”¨å…¶ä»–æ¨¡å‹ï¼‰ï¼Œç¥ç»ç½‘ç»œä¸æ˜¯å·ç§°å­¦ä¹ èƒ½åŠ›å¼ºå—ï¼Œå‡†ç¡®ç‡å¿…é¡»é«˜äºåŸºæ¨¡å‹ã€‚
def svm_baseline():
    training_data, validation_data, test_data = load_data()

    clf = svm.SVC()
    clf.fit(training_data[0], training_data[1])

    predictions = [int(a) for a in clf.predict(test_data[0])]
    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))
    # zipåï¼Œä¸¤ä¸¤å¯¹åº”ä¸€ä¸ªå…ƒç»„ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªè¿”å›çš„æ˜¯ä¸€ä¸ªå¯¹è±¡ã€‚å¦‚éœ€å±•ç¤ºåˆ—è¡¨ï¼Œéœ€æ‰‹åŠ¨ list() è½¬æ¢ï¼Œæˆ–è€…for
    # int(True)=1, int(False)=0,ä¾¿äºåé¢sumè®¡ç®—æ±‚å’Œ

    print("Baseline classifier using an SVM.")
    print(str(num_correct) + " of " + str(len(test_data[1])) + " values correct.")
#     10000æ¡æ•°æ®ï¼Œ9214æ¡åˆ†ç±»æ­£ç¡®ï¼Œæ­£ç¡®ç‡92.14%ï¼Œå¦‚æœç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼Œ94.35%

# svm_baseline()


#----------------- å°è£…è®­ç»ƒæ•°æ®é›†ã€éªŒè¯æ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›† --------------
# åªæœ‰è®­ç»ƒé›†éœ€è¦æŠŠæ ‡ç­¾one-hotç¼–ç 
def load_data_wrapper():
    tr_d, va_d, te_d = load_data()

    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]  # è®­ç»ƒé›†çš„æ¯ä¸€ä¸ªæ ·æœ¬è½¬æ¢æˆ784è¡Œï¼Œ1åˆ—çš„åˆ—è¡¨ï¼Œåˆ—è¡¨é‡Œé¢æ˜¯æ•°ç»„æ ¼å¼
    training_results = [vectorized_result(y) for y in tr_d[1]]  # è®­ç»ƒé›†çš„æ¯ä¸€ä¸ªæ ‡ç­¾one-hotç¼–ç 

    training_data = zip(training_inputs, training_results)  # å°è£…è®­ç»ƒé›†çš„æ ·æœ¬å’Œæ ‡ç­¾

    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]
    validation_data = zip(validation_inputs, va_d[1])  # å°è£…éªŒè¯é›†çš„æ ·æœ¬å’Œæ ‡ç­¾

    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]
    test_data = zip(test_inputs, te_d[1])  # å°è£…æµ‹è¯•é›†çš„æ ·æœ¬å’Œæ ‡ç­¾

    return training_data, validation_data, test_data


def vectorized_result(j):
    e = np.zeros((10, 1))  # 10è¡Œ1åˆ—çš„äºŒç»´æ•°ç»„
    e[j] = 1.0
    return e


#------------------------- å®šä¹‰å‰é¦ˆç¥ç»ç½‘ç»œæ¨¡å‹ ------------------------------------

class QuadraticCost(object):
    # æŸå¤±å‡½æ•°ä¸ºå¹³æ–¹å·®æŸå¤±
    @staticmethod
    def fn(a, y):
        # å¹³æ–¹æŸå¤±å‡½æ•°
        return 0.5 * np.linalg.norm(a - y) ** 2

    @staticmethod
    def delta(z, a, y):
        # å¹³æ–¹æŸå¤±ä¸‹çš„è¾“å‡ºå±‚è¯¯å·®
        return (a - y) * sigmoid_prime(z)


class CrossEntropyCost(object):
    """
        @staticmethod é™æ€æ–¹æ³•åªæ˜¯åä¹‰ä¸Šå½’å±ç±»ç®¡ç†ï¼Œä½†æ˜¯ä¸èƒ½ä½¿ç”¨ç±»å˜é‡å’Œå®ä¾‹å˜é‡ï¼Œæ˜¯ç±»çš„å·¥å…·åŒ…
        æ”¾åœ¨å‡½æ•°å‰ï¼ˆè¯¥å‡½æ•°ä¸ä¼ å…¥selfæˆ–è€…clsï¼‰ï¼Œæ‰€ä»¥ä¸èƒ½è®¿é—®ç±»å±æ€§å’Œå®ä¾‹å±æ€§
    """
    @staticmethod
    def fn(a, y):
        # å•ä¸ªå®ä¾‹xçš„æŸå¤±å‡½æ•°Cxä¸ºäº¤å‰ç†µæŸå¤±å‡½æ•°
        return np.sum(np.nan_to_num(-y * np.log(a) - (1 - y) * np.log(1 - a)))

    # numpy.nan_to_num(x):
    # ä½¿ç”¨0ä»£æ›¿æ•°ç»„xä¸­çš„nanå…ƒç´ ï¼Œä½¿ç”¨æœ‰é™çš„æ•°å­—ä»£æ›¿infå…ƒç´ 

    @staticmethod
    def delta(z, a, y):   # æ”¹é™æ€æ–¹æ³•å‡½æ•°é‡Œä¸ä¼ å…¥self æˆ– cls
        """
        :param z: zs[-1]
        :param a: activations[-1]
        :param y: y
        :return: é¢„æµ‹è¾“å‡ºå€¼ - å®é™…è¾“å‡ºå€¼ = äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸‹çš„è¾“å‡ºå±‚è¯¯å·®
        """
        return a - y


class Network2(object):

    def __init__(self, sizes, cost=CrossEntropyCost):  # Network2([784, 30, 10], cost=CrossEntropyCost)
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.cost = cost
        self.default_weight_initializer()

    def default_weight_initializer(self):
        """
        æƒå€¼åˆå§‹åŒ–:è®¾å¯¹ ğ‘™ å±‚æœ‰æœ‰ ğ‘›ä¸ªè¾“å…¥ç¥ç»å…ƒï¼Œä½¿ç”¨å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º 1/sqrt(n)çš„é«˜æ–¯åˆ†å¸ƒåˆå§‹åŒ– ğ‘™ å±‚çš„æƒå€¼ã€‚
        ä¸æ­£å¤ªåˆ†å¸ƒåˆå§‹åŒ–çš„ç›¸æ¯”ä¼˜ç‚¹ï¼šæ”¹å–„ç½‘ç»œçš„å­¦ä¹ é€Ÿåº¦
        """
        self.weights = [np.random.randn(y, x) / np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])]
        # self.weights[0]ç¬¬äºŒå±‚çš„åˆå§‹åŒ–æƒé‡ï¼Œ(30, 784)
        # self.weights[1]ä¸ºç¬¬ä¸‰å±‚çš„åˆå§‹åŒ–æƒé‡ï¼Œ(10, 30)

        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]
        # biases[0]ä¸ºç¬¬äºŒå±‚çš„åˆå§‹åŒ–åç½®  (30, 1)
        # biases[1]ä¸ºç¬¬ä¸‰å±‚çš„åˆå§‹åŒ–åç½®   (10, 1)

    def feedforward(self, a):
        """
        :param a: è®­ç»ƒæ•°æ®çš„å•ä¸ªå®ä¾‹ï¼ˆä¸åŒ…æ‹¬labelï¼‰
        :return: å‰å‘ä¼ æ’­æ±‚a3(é¢„æµ‹è¾“å‡ºy_hat)
        """
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a) + b)
        return a

    def SGD(self, training_data, epochs, mini_batch_size, eta,
            lmbda=0.0,
            evaluation_data=None,
            monitor_evaluation_cost=False,
            monitor_evaluation_accuracy=False,
            monitor_training_cost=False,
            monitor_training_accuracy=False,
            early_stopping_n=0):
        """
        :param training_data: è®­ç»ƒæ ·æœ¬ï¼šåŒ…æ‹¬å®ä¾‹å’Œå‘é‡åŒ–çš„labelï¼Œzipæ ¼å¼
        :param epochs: 30ä¸ªå¾ªç¯
        :param mini_batch_size: æ¯ç»„10ä¸ªæ ·æœ¬
        :param eta: å­¦ä¹ ç‡ 0.5
        :param lmbda: æ­£åˆ™åŒ–ç³»æ•° 5.0
        :param evaluation_data: validation_data
        :param monitor_evaluation_cost: True
        :param monitor_evaluation_accuracy: True
        :param monitor_training_cost: True
        :param monitor_training_accuracy: True
        :param early_stopping_n: 0è¡¨ç¤ºä¸é‡‡ç”¨æ—©åœæ³•ï¼Œå¤§äº0è¡¨ç¤ºé‡‡ç”¨æ—©åœæ³•ï¼šå®šä¹‰éªŒè¯é›†ä¸æ›´æ–°å¤šå°‘æ¬¡å°±åœæ­¢è®­ç»ƒ
        :return: å®šä¹‰æ‰¹æ¬¡éšæœºæ¢¯åº¦ä¸‹é™ï¼Œ
                è¿”å›evaluation_cost, evaluation_accuracy, training_cost, training_accuracy
        """

        training_data = list(training_data)  # zipæ‰“åŒ…ä¸ºå…ƒç»„çš„åˆ—è¡¨,ä½†æ˜¯æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œéœ€è¦å°†zipæ ¼å¼åˆ—è¡¨åŒ–
        n = len(training_data)               # 5ä¸‡ä¸ª

        if evaluation_data:
            evaluation_data = list(evaluation_data)
            n_data = len(evaluation_data)

        # é‡‡ç”¨æ—©åœæ³•ç”¨åˆ°
        best_accuracy = 0
        # best_accuracy = 1
        no_accuracy_change = 0

        # 30ä¸ªå¾ªç¯è®°å½•
        training_cost, training_accuracy = [], []
        evaluation_cost, evaluation_accuracy = [], []

        for j in range(epochs):  # 30æ¬¡å¾ªç¯
            random.shuffle(training_data)   # æ‰“ä¹±è®­ç»ƒé›†æ ·æœ¬çš„é¡ºåº

            # n=5ä¸‡ä¸ªæ•°æ®é›†,
            # mini_batch_size(10ä¸ªæ ·æœ¬ä½œä¸ºä¸€ä¸ªä¸€ç»„)ï¼Œä¸€å…±æœ‰n/mini_batch_size=5ä¸‡/10=5kç»„,æ¯ç»„10ä¸ªæ ·æœ¬
            mini_batches = [training_data[k: k + mini_batch_size] for k in range(0, n, mini_batch_size)]
            # print(len(mini_batches))   # 5kç»„

            # æ¯ä¸€ç»„mini_batch10ä¸ªæ ·æœ¬
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))

            print("-"*100)
            print("Epoch %s training complete" % j)

            # è®­ç»ƒé›†æŸå¤±ï¼ˆæ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°ï¼‰
            if monitor_training_cost:
                cost = self.total_cost(training_data, lmbda)  # ä¸€æ¬¡å¾ªç¯
                training_cost.append(cost)  # 30æ¬¡å¾ªç¯
                print("Cost on training data: {}".format(cost))

            # è®­ç»ƒé›†å‡†ç¡®ç‡
            if monitor_training_accuracy:
                accuracy = self.accuracy(training_data, convert=True)  # ä¸€æ¬¡å¾ªç¯
                training_accuracy.append(accuracy)   # 30æ¬¡å¾ªç¯
                print("Accuracy on training data: {} / {}".format(accuracy, n))

            # éªŒè¯é›†æŸå¤±ï¼ˆæ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°ï¼‰
            if monitor_evaluation_cost:
                cost = self.total_cost(evaluation_data, lmbda, convert=True)  # ä¸€æ¬¡å¾ªç¯
                evaluation_cost.append(cost)  # 30æ¬¡å¾ªç¯
                print("Cost on evaluation data: {}".format(cost))

            # éªŒè¯é›†å‡†ç¡®ç‡
            if monitor_evaluation_accuracy:
                accuracy = self.accuracy(evaluation_data)  # ä¸€æ¬¡å¾ªç¯
                evaluation_accuracy.append(accuracy)   # 30æ¬¡å¾ªç¯
                print("Accuracy on evaluation data: {} / {}".format(self.accuracy(evaluation_data), n_data))

            # å½“éªŒè¯é›†ä¸Šçš„é”™è¯¯ç‡ä¸å†ä¸‹é™æ—¶ï¼Œå°±åœæ­¢è¿­ä»£
            if early_stopping_n > 0:
                """
                    é‡‡ç”¨æ—©åœæ³•:
                """
                # å¦‚æœå½“å‰epochåœ¨éªŒè¯é›†ä¸Šçš„accuracyå¤§äºå‰é¢çš„æœ€å¥½accuracy,è®°å½•
                if accuracy > best_accuracy:
                    best_accuracy = accuracy
                    no_accuracy_change = 0
                    print("Early-stopping: Best so far {}".format(best_accuracy))

                # å¦åˆ™è®°å½•no_accuracy_changeçš„æ¬¡æ•°
                else:
                    no_accuracy_change += 1

                #  å¦‚æœå‡†ç¡®ç‡ä¸å†ä¸Šå‡ç´¯åŠ çš„æ¬¡æ•°ç­‰äºè§„å®šçš„æ¬¡æ•°ï¼Œå¼ºåˆ¶è¿”å›
                if (no_accuracy_change == early_stopping_n):
                    print("Early-stopping: No accuracy change in last epochs: {}".format(early_stopping_n))
                    return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy

        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy

    def update_mini_batch(self, mini_batch, eta, lmbda, n):   # self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))
        """
        :param mini_batch: 10ä¸ªæ ·æœ¬
        :param eta: 0.5çš„å­¦ä¹ ç‡
        :param lmbda: æ­£åˆ™åŒ–ç³»æ•°
        :param n: è®­ç»ƒæ ·æœ¬æ•°ï¼Œ5ä¸‡
        :return: æ¯ä¸€ç»„mini_batchæ›´æ–°å½“å‰çš„æƒé‡å’Œåç½® ï¼Œå³æ¯10ä¸ªæ ·æœ¬æ›´æ–°ä¸€æ¬¡bå’Œw
        """
        nabla_b = [np.zeros(b.shape) for b in self.biases]   # ä¾æ¬¡æ˜¯ç¬¬äºŒå±‚ï¼Œç¬¬ä¸‰å±‚çš„åç½®
        nabla_w = [np.zeros(w.shape) for w in self.weights]  # ä¾æ¬¡æ˜¯ç¬¬äºŒå±‚ï¼Œç¬¬ä¸‰å±‚çš„æƒé‡

        for x, y in mini_batch:

            # å¯¹10ä¸ªæ ·æœ¬ï¼Œå–æ¯ä¸ªæ ·æœ¬çš„xç‰¹å¾å‘é‡, yæ ‡ç­¾ï¼ˆone-hotï¼‰
            # xä¸º784è¡Œï¼Œ1åˆ—çš„å‘é‡ï¼Œyä¸º10è¡Œä¸€åˆ—çš„æ ‡ç­¾
            # ä¾æ¬¡å¯¹10ä¸ªæ ·æœ¬çš„ä¸¤å±‚ï¼Œè®¡ç®—æŸå¤±å‡½æ•°åœ¨å„å±‚çš„åç½®æ¢¯åº¦ç´¯åŠ ã€æƒé‡æ¢¯åº¦ç´¯åŠ 

            delta_nabla_b, delta_nabla_w = self.backprop(x, y)  # æŸå¤±å‡½æ•°å¯¹æ¯ä¸€ä¸ªæ ·æœ¬å„å±‚çš„å…³äºbã€wçš„æ¢¯åº¦
            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]  # 10ä¸ªæ ·æœ¬ï¼Œå…³äºåç½®æ¢¯åº¦ç´¯åŠ 
            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]  # 10ä¸ªæ ·æœ¬ï¼Œå…³äºæƒé‡æ¢¯åº¦ç´¯åŠ 

        # æƒé‡wè€ƒè™‘äº†æ­£åˆ™åŒ–,æ³¨æ„è¿™é‡Œç”¨å‰é¢ç”¨ n ï¼Œè€Œä¸æ˜¯len(mini_batch)
        self.weights = [(1 - eta * (lmbda / n)) * w - (eta / len(mini_batch)) * nw
                        for w, nw in zip(self.weights, nabla_w)]

        # åç½®bå¹¶ä¸èƒ½å¢åŠ æ¨¡å‹å¤æ‚åº¦ ä¸éœ€è¦æ­£åˆ™åŒ–
        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]

    def backprop(self, x, y):
        """
            å‰å‘ä¼ æ’­è®¡ç®—
            è¯¯å·®çš„åå‘ä¼ æ’­è®¡ç®—
            è¾“å…¥ï¼š
                x è®­ç»ƒå®ä¾‹çš„æ¯ä¸€ä¸ªæ ·æœ¬ï¼Œ784*1
                y æ ‡ç­¾ï¼Œ10*1
            è¾“å‡ºï¼š æŸå¤±å‡½æ•°å¯¹æ¯ä¸€ä¸ªæ ·æœ¬å„å±‚çš„å…³äºbã€wçš„æ¢¯åº¦
        """
        # å ä½
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        activation = x      # æ ·æœ¬çš„784ä¸ªç»´åº¦ï¼Œ784*1
        activations = [x]   # åˆ†å±‚å­˜å‚¨æ¯å±‚çš„è¾“å‡ºé¡¹ï¼ˆzæ¿€æ´»ï¼‰,aå‘é‡, ä¾æ¬¡æ˜¯ç¬¬ä¸€å±‚a1ï¼ˆ784*1ï¼‰, ç¬¬äºŒå±‚a2ï¼ˆ30*1ï¼‰ï¼Œ ç¬¬ä¸‰å±‚a3ï¼ˆ10*1ï¼‰
        zs = []             # åˆ†å±‚å­˜å‚¨æ¯å±‚çš„ z å‘é‡,  ä¾æ¬¡æ˜¯ç¬¬äºŒå±‚z2ï¼ˆ30*1ï¼‰ï¼Œç¬¬ä¸‰å±‚z3ï¼ˆ10*1ï¼‰

        # å‰å‘è®¡ç®—
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation) + b    # ç¬¬2å±‚ç»´åº¦ï¼š30*1ï¼Œç¬¬3å±‚ç»´åº¦ï¼š10*1, numpyç±»å‹
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)

        #  ç¬¬3å±‚è¯¯å·®ï¼šdelta3ï¼ˆåˆ—å‘é‡ä¸åˆ—å‘é‡çš„å†…ç§¯ï¼‰ï¼Œ10*1
        delta = self.cost.delta(zs[-1], activations[-1], y)
        nabla_b[-1] = delta  # æŸå¤±å‡½æ•°åœ¨ç¬¬ä¸‰å±‚å…³äºåç½®çš„æ¢¯åº¦ï¼Œç­‰äºç¬¬ä¸‰å±‚çš„è¯¯å·®ï¼Œåˆ—å‘é‡, 10*1
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        # å…³äºæƒé‡çš„æ¢¯åº¦ï¼Œ10*30ï¼ŒæŸå¤±å‡½æ•°åœ¨æœ¬å±‚å…³äºæƒå€¼çš„æ¢¯åº¦ = æœ¬å±‚è¯¯å·® * ä¸Šä¸€å±‚çš„æ¿€æ´»è¾“å‡º

        # ç¬¬2å±‚è¯¯å·®ï¼šdelta2ï¼Œ30*1
        for m in range(2, self.num_layers):
            z = zs[-m]  # ç¬¬2å±‚z2
            sp = sigmoid_prime(z)   # ç¬¬2å±‚è¯¯å·®çš„å‰åŠéƒ¨åˆ†(å½“å‰å±‚çš„zä»£å…¥sigmoidå‡½æ•°å†æ±‚å¯¼)ï¼Œåˆ—å‘é‡å½¢å¼ï¼Œ30*1
            delta = np.dot(self.weights[-m + 1].transpose(), delta) * sp  # ç¬¬äºŒå±‚è¯¯å·®ï¼Œ30*1 x 30*1 = 30*1(å†…ç§¯)
            nabla_b[-m] = delta     # æŸå¤±å‡½æ•°åœ¨ç¬¬2å±‚å…³äºåç½®çš„æ¢¯åº¦ï¼Œä¸ºç¬¬2å±‚çš„è¯¯å·®ï¼Œ30*1
            nabla_w[-m] = np.dot(delta, activations[-m - 1].transpose())  # å…³äºæƒé‡çš„æ¢¯åº¦,30*1 x 1*784 = 30*784

        # è¿”å›æ¢¯åº¦å€¼
        return nabla_b, nabla_w

    # è®¡ç®—å‡†ç¡®ç‡
    def accuracy(self, data, convert=False):
        if convert:
            # é’ˆå¯¹è®­ç»ƒé›† self.accuracy(training_data, convert=True)
            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]
        else:
            # é’ˆå¯¹éªŒè¯é›†æˆ–æµ‹è¯•é›† self.accuracy(evaluation_data)
            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]

        result_accuracy = sum(int(x == y) for (x, y) in results)
        return result_accuracy

    # è®¡ç®—è®­ç»ƒé›†ä¸Šçš„æŸå¤±ï¼Œè¿™ä¼šåŠ å…¥L2è§„èŒƒåŒ–
    # å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„L1è§„èŒƒåŒ–å®ç°çš„é—®é¢˜ï¼Œè¿™é‡Œä»£ç å¯æ”¹æˆ
    # cost += (lmbda / len(data)) * sum(np.linalg.norm(w) for w in self.weights)
    def total_cost(self, data, lmbda, convert=False):
        """
        å¯¹äºè®­ç»ƒé›†ï¼Œlabelå·²ç»æ˜¯å‘é‡åŒ–ï¼Œæ‰€ä»¥convert=Falseï¼Œ self.total_cost(training_data, lmbda)
        å¯¹äºéªŒè¯é›†ï¼Œlabelä¸æ˜¯å‘é‡åŒ–ï¼Œ æ‰€ä»¥convert=Trueï¼Œ  self.total_cost(evaluation_data, lmbda, convert=True)
        labelå‘é‡åŒ–æ˜¯ä¸ºäº†è®¡ç®—æŸå¤±å‡½æ•°self.cost.fn
        :return: å…¨éƒ¨æ•°æ®çš„æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°
        """
        cost = 0.0

        for x, y in data:
            a = self.feedforward(x)        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„a3
            if convert:
                y = vectorized_result(y)
            # å•ä¸ªå®ä¾‹xçš„æŸå¤±å‡½æ•°ï¼Œæ‰€æœ‰è®­ç»ƒæ•°æ®çš„å®ä¾‹æ±‚å’Œ
            cost += self.cost.fn(a, y) / len(data)
            # æ­£åˆ™åŒ–ç›®æ ‡å‡½æ•°
            cost += 0.5 * (lmbda / len(data)) * sum(np.linalg.norm(w) ** 2 for w in self.weights)

        return cost

    def evaluate(self, test_data):
        """
            è¾“å…¥ï¼šzipæ ¼å¼çš„æµ‹è¯•æ•°æ®
            è¿‡ç¨‹ï¼šæ¯ä¸ªæµ‹è¯•æ ·æœ¬è®¡ç®—å‰å‘ä¼ æ’­åˆ°a3(y_hat),å–y_hatå¾—æœ€å¤§å€¼çš„ç´¢å¼•ä¸ºé¢„ä¼°å€¼
            è¾“å‡ºï¼šåˆ†ç±»æ­£ç¡®çš„ä¸ªæ•°
        """
        # print(type(test_data))
        # print(list(test_data))
        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]  # argmaxè¿”å›çš„æ˜¯æœ€å¤§æ•°çš„ç´¢å¼•
        # print(list(test_data))
        return sum(int(x == y) for (x, y) in test_results)

    # ä¿å­˜ç¥ç»ç½‘ç»œæ–‡ä»¶filenameï¼Œä¿å­˜ç½‘ç»œçš„ç»“æ„ï¼Œæƒé‡ï¼Œåç½®ï¼Œä½¿ç”¨çš„æŸå¤±å‡½æ•°ã€‚
    def save(self, filename):
        """
        sizeæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œ[784, 30, 10]
        wæ˜¯numpyçš„arrayç±»å‹ï¼Œè°ƒç”¨å®ƒçš„tolistæ–¹æ³•ï¼ŒæŠŠå®ƒè½¬æ¢æˆpythonçš„åˆ—è¡¨ç±»å‹
        è¿™é‡Œä¿å­˜çš„æ˜¯costçš„ç±»åå­—ã€‚(CrossEntropyCost)
        """
        data = {"sizes": self.sizes,
                "weights": [w.tolist() for w in self.weights],
                "biases": [b.tolist() for b in self.biases],
                "cost": str(self.cost.__name__)}
        f = open(filename, "w")
        json.dump(data, f)
        f.close()


# ä»filenameåŠ è½½ç¥ç»ç½‘ç»œï¼Œè¿”å›ä¸€ä¸ªç¥ç»ç½‘ç»œå®ä¾‹
def load(filename, test_data):
    """
    :param filename: æ‰“å¼€æ–‡ä»¶è·¯å¾„
    :param test_data: æµ‹è¯•æ•°æ®é›†ï¼Œzipæ ¼å¼
    :return: æµ‹è¯•é›†é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•°
    è¿™é‡Œç”¨æœ‰ä¸ªå‘ï¼Œzipå¯¹è±¡â€ æ˜¯ä¸€ä¸ªè¿­ä»£å™¨ã€‚ è¿­ä»£å™¨åªèƒ½å‰è¿›ï¼Œä¸èƒ½åé€€ã€‚
    æ‰€ä»¥net.evaluate(test_data)ä¹‹åè¿è¡Œlist(test_data)ä¸º0
    """

    f = open(filename, "r")
    data = json.load(f)   # jsonçš„loadæ–¹æ³•å°†å­—ç¬¦ä¸²è¿˜åŸä¸ºæˆ‘ä»¬çš„å­—å…¸
    f.close()

    # test_n = test_data
    # test_n = len(list(test_n))

    # cost = getattr(sys.modules[__name__], data["cost"])
    cost = data["cost"]
    net = Network2(data["sizes"], cost=cost)
    net.weights = [np.array(w) for w in data["weights"]]
    net.biases = [np.array(b) for b in data["biases"]]
    test_data_accuracy = net.evaluate(test_data)

    print("æµ‹è¯•é›† {} / {}".format(test_data_accuracy, 10000))
    return net


def sigmoid(z):
    return 1.0 / (1.0 + np.exp(-z))


def sigmoid_prime(z):
    """
        è¾“å…¥ï¼šç¬¬ä¸‰å±‚çš„åˆ—å‘é‡z3,10è¡Œ1åˆ—ï¼Œnumpyæ ¼å¼
              sigmoid_prime(zs[-1])

        è¾“å‡ºï¼šz3ä»£å…¥sigmoidå‡½æ•°ï¼Œå†æ±‚å¯¼ã€‚è¾“å‡ºå±‚è¯¯å·®çš„ååŠéƒ¨åˆ†,10è¡Œ1åˆ—ï¼Œnumpyæ ¼å¼
    """
    return sigmoid(z) * (1 - sigmoid(z))


if __name__ == '__main__':

#------------------------- æ˜¾ç¤ºæ•°æ®é›†ç»´åº¦ ------------------------------------
  # training_data, validation_data, test_data = load_data()
  # image = training_data[0][0].reshape(28, 28)  # è®­ç»ƒé›†labelçš„ç¬¬ä¸€ä¸ªå›¾ç‰‡ï¼Œæ¯ä¸ªæ ·æœ¬ï¼ˆå›¾ç‰‡ï¼‰28è¡Œï¼Œ28åˆ—ï¼Œå³784ç»´ç‰¹å¾
  #
  # print(training_data[0].shape, training_data[1].shape)
  # print(validation_data[0].shape, validation_data[1].shape)
  # print(test_data[0].shape, test_data[1].shape)


#------------------------- å°è£…æ•°æ®é›†æˆzipæ ¼å¼ ------------------------------------
  training_data, validation_data, test_data = load_data_wrapper()  # ä¸‰ä¸ªæ•°æ®é›†éƒ½æ˜¯zipæ ¼å¼


#----------------------------- è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ -------------------------------------

  # æ„å»ºè¾“å…¥å±‚784ä¸ªç»“ç‚¹ï¼Œéšè—å±‚30ä¸ªç»“ç‚¹ï¼Œè¾“å‡ºå±‚10ä¸ªç»“ç‚¹çš„ä¸‰å±‚å‰é¦ˆç¥ç»ç½‘ç»œï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®é›†è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®é›†è¿›è¡Œæµ‹è¯•ï¼š
  net = Network2([784, 30, 10], cost=CrossEntropyCost)
  net.SGD(training_data, 1, 10, 0.5, lmbda=5.0, evaluation_data=validation_data,
         monitor_evaluation_cost=True,
         monitor_evaluation_accuracy=True,
         monitor_training_cost=True,
         monitor_training_accuracy=True)

  net.save('./data/save_model')

#----------------------------- è°ƒç”¨æ¨¡å‹åéªŒè¯æµ‹è¯•é›† -------------------------------------
  load('./data/save_model', test_data)