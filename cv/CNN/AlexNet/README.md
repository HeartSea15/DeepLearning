# AlexNet提出的新的技术

1：成功使用ReLU作为CNN的激活函数，在较深的网络中效果超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题

2：训练时使用Dropout随机忽略一部分神经元，以避免过拟合。最后几个全连接层使用Dropout

3：在CNN中使用重叠的最大池化，避免平均池化的模糊化效果。步长比池化核的尺寸小，池化层的输出之间会有重叠和覆盖，提升了特征的丰富性

4：提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力

5：使用CUDA加速深度卷积网络的训练

6：数据增强，随机地从256 * 256 的原始图像中截取224 * 224大小的区域（以及水平翻转的镜像），相当于增加了（256-224）^2*2=2048倍的数据量。
进行预测时，取图片的四个角加中间共5个位置，并进行左右翻转，一共获得10张图片，最后结果取平均。同时对图像的RGB数据进行PCA处理，
并对主成分做一个标准差为0.1的高斯扰动，可以使错误率降1%

## train_test.py：用真实图片集数据实现的AlexNet，训练加预测，待看
