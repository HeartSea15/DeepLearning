# TensorFlow实现Google Inception Net
## Google Inception Net介绍
· Google Inception Net首次出现在ILSVRC 2014的比赛中，以较大的优势获得了第一名，那届比赛中的Inception Net通常被称为Inception V1，它最大的特点是控制了计算量和参数量的同时，获得了非常好的分类性能--top-5错误率6.67%。

· Inception V1有22层深，比AlexNet的8层或者VGGNet的19层还要深，但其计算量只有15亿次浮点运算，只有500万的参数量。

## Inception V1降低参数量的目的
（1）参数越大模型越庞大，需要供模型学习的数据量就越大，而目前高质量的数据非常昂贵。

（2）参数越多，耗费的计算资源也会更大。

## Inception V1参数少但效果好的原因
· 模型层数更深、表达能力更强
· 去除了最后的全连接层，用全局平均池化层(即将图片尺寸变为1x1）来取代它。
· Inception V1中精心设计的Inception Module提高了参数的利用率。后两点都借鉴了NIN论文中的思想。形象解释就是Inception Module本身如同大网络中的一个小网络，其结构可以反复堆叠在一起形成大网络。

## 1×1卷积在Google Inception Net被频繁应用的原因
· 可以跨通道组织结构，可以对输出通道升维和降维。1×1卷积可以很自然的把相关性很高的、在同一个空间位置但是不同通道的特征连接在一起。

## Inception V3中Inception Module的结构

一般情况下有4个分支：

第1个分支一般是1x1卷积，

第2个分支一般是1x1卷积再接分角后（factorized)的1xn和nx1卷积，

第3个分支和第2分支类似，但是一般更深一些，

第4个分支一般具有最大池化或平均池化。

因此，Inception Module是通过组合比较简单的特征抽象（分支1）、比较复杂的特征抽象（分支2和分支3）和一个简化结构的池化层（分支4），一共4种不同程序的特征抽象和变换来有选择地保留不同层次的高阶特征，这样可以最大程序地丰富网络的表达能力

## 正则化方法
· 增大数据集数量，如：data augmentation

· 调整学习率

· L2正则

· dropout

· LRN

· VGGNet提出堆叠小型卷积核。如两个串联的3×3卷积核相当于一个5×5的卷积核，降低参数量并减轻过拟合

· Inception V1有22层深，其中间节点的分类效果也很好。在网络中，使用到了辅助分类节点，即将中间某一层的输出用作分类，并按一个较小的权重（0.4）加到最终的分类结果中，相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，top-5错误率6.67%。

· Inception V2提出了著名的Batch Normalization（以下简称BN）方法。BN在用于神经网络某层时，会对每一个mini-batch数据的内部进行标准化（normalization）处理，使输出规范化到N(0,1)的正态分布，减少了Internal Covariate Shift（内部神经元分布的改变）。BN的论文指出，传统的深度神经网络在训练时，每一层的输入的分布都在变化，导致训练变得困难，我们只能使用一个很小的学习速率解决这个问题。而对每一层使用BN之后，我们就可以有效地解决这个问题，学习速率可以增大很多倍。而达到之前的准确率后，可以继续训练，并最终取得远超于Inception V1模型的性能——top-5错误率4.8%，已经优于人眼水平。因为BN某种意义上还起到了正则化的作用，所以可以减少或者取消Dropout，简化网络结构。

调整：

（1）增大学习速率并加快学习衰减速度以适用BN规范化后的数据；

（2）去除Dropout并减轻L2正则（因BN已起到正则化的作用）；

（3）去除LRN；

（4）更彻底地对训练样本进行shuffle；

（5）减少数据增强过程中对数据的光学畸变（因为BN训练更快，每个样本被训练的次数更少，因此更真实的样本对训练更有帮助）。

在使用了这些措施后，Inception V2在训练达到Inception V1的准确率时快了14倍，并且模型在收敛时的准确率上限更高

· Inception V3引入了Factorization into small convolutions的思想。将3×3卷积拆成1×3卷积和3×1卷积，节约参数，加速运算并减轻了过拟合（比将7×7卷积拆成1×7卷积和7×1卷积，比拆成3个3´3卷积更节约参数），同时增加了一层非线性扩展模型表达能力。

